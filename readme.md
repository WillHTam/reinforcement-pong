# Deep Q Learning with Pong
- Takes about 3 hours with a GTX1070

## Files
- `deep_q_pong.py` - main module with training loop, loss function, and experience replay buffer
- `q_play.py` - use saved weights to play an episode
- `/lib/q_model.py` - DQN neural net with architecture from the DeepMind Nature paper
    - “Human-Level Control Through Deep Reinforcement Learning doi:10.1038/nature14236, Mnih and others.”
- `/lib/atari_wrappers.py`
    - Atari wrappers from https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
- `/lib/wrappers.py`
    - Only the needed wrappers from the original atari_wrappers file

## Notes
- The amount of observable states and combination of values is impossibly large to track and approximate values for.  Each observation will produce 100k values which require transformation.
- In essence, this will be a regression problem to learn a nonlinear representation that maps state and action onto a value.
- Exploitation vs Exploration
    - Need to explore the environment, but using random actions is a waste if we already know their outcomes
    - Random behavior is better at the beginning to explore the environment with a uniform distribution
    - However as more about the environment becomes known, it's better to use the Q approximation to decide an action
    - So implement /epsilon-greedy/
        - A method which switches between the extremes of random actions and Q policy using the hyperparameter epsilon.
        - Usual practice is to start with 100% random actions, and slowly decrease it to 2~5% random actions.
        - This method will help to explore the environment in the beginning, then switch to good policy when the agent is trained.

- Using Stochastic Gradient Descent
    - This reinforcement problem is able to use a supervised technique.
        - By using the Bellman equation to calculate targets, the loss can be calculated for the fitting of the nonlinear function Q(s,a)
    - SGD optimization requires training data that is i.i.d. (independent and identically distributed)
        - However 1) the samples are not independent, they are very close to each other being in the same episode
        - and 2) Since the optimal policy is not yet known, its data distribution cannot be generated.
        - To deal with this problem, use a replay buffer.  Instead of using latest experience, sample training data from a large store/buffer of past experience. This buffer will be of fixed size, where new data pushes old data out.  This gives somehwat independent data, but recent enough to train on samples generated by the current policy.  

- An image from Pong doesn't capture important information like speed/direction of the ball and paddles.
    - This means Pong is in the area of Partially Observable Markov Decision Processes
    - Partially solve this problem by stacking subsequent frames, from which the agent can deduce speed/direction
- To make training more stable, use a Target Network
    - State s and s' only have one step between them which makes them hard for the network to differentiate.
    - Updating Q(s, a) may affect Q(s', a') which spoils the model's ability to converge
    - Using a target network means keeping a copy of the network and using that for Q(s', a') in the Bellman equation.
    - This update is only done periodically, once in N steps  (usually 1000 to 10000 iterations)

## Deep Q Learning
1. Initialize parameters for Q(s,a) and Qhat(s, a) with random weights, initialize epsilon to 1.0 (all random) and empty replay buffer
2. With probability epsilon, select a random action a, otherwise a = argmax(Q)
3. Execute action a in the emulator and observe reward r and the next state s'
4. Store transition (s, a, r, s') in the replay buffer
5. Sample a random minibatch of transitions from the replay buffer.
6. For every transition in the buffer, calculate target y=r if the episode has ended at this step.
    - Otherwise calculate y = r + gamma * Q(s', a')
7. Calculate loss: L = (Q(s,a) - y)**2
8. Update Q(s, a) using SGD algorithm
9. Every N steps copy weights from Q to Qhat
10. Repeat from Step 2 until model has converged